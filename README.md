This repository explores Responsible AI techniques â€” focusing on Bias Detection & Mitigation, Machine Unlearning, and other methods to build fair, transparent, and trustworthy AI systems.
It contains modular projects that showcase research and practical implementations across different aspects of AI fairness, explainability, and accountability.

ğŸ“‚ Repository Structure
responsible-ai-lab/
â”‚
â”œâ”€â”€ bias-detection-mitigation/    # Fairness evaluation & mitigation in ML models
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ machine-unlearning/           # Forgetting learned data in neural networks
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ requirements.txt              # Common dependencies
â””â”€â”€ README.md                     # Main repo documentation

ğŸš€ Projects
ğŸ”¹ Bias Detection & Mitigation

Dataset: UCI Adult Dataset (Income Prediction)

Models: Logistic Regression, Decision Trees

Tools: AI Fairness 360 (AIF360)

Features:

Measured fairness metrics (Disparate Impact, Statistical Parity Difference)

Applied reweighting techniques to reduce bias

Visualized fairness vs accuracy trade-offs

Results: Reduced bias while maintaining ~85% accuracy.

ğŸ”¹ Machine Unlearning with CNN on MNIST

Task: Train CNN on MNIST and then selectively remove digit 7.

Features:

CNN trained on full dataset

Unlearning process: retrained model without class 7

Compared accuracy before & after unlearning

Results: Accuracy impact visualized in plots; models saved before/after unlearning.

Future Work: Federated Unlearning, Differential Privacy.

ğŸ›  Tech Stack

Languages: Python

ML/DL Libraries: TensorFlow, Scikit-learn

Fairness & XAI: AI Fairness 360 (AIF360), SHAP, LIME (planned)

Utilities: NumPy, Pandas, Matplotlib

âš™ï¸ Installation & Setup

1ï¸âƒ£ Clone this repository

git clone https://github.com/your-username/responsible-ai-lab.git
cd responsible-ai-lab


2ï¸âƒ£ Install dependencies

pip install -r requirements.txt


3ï¸âƒ£ Explore individual projects inside their folders

ğŸ“ˆ Future Directions

âœ… Bias detection & mitigation in ML models

âœ… Machine unlearning with CNNs

ğŸ”œ Explainability across modalities (SHAP, LIME, Grad-CAM)

ğŸ”œ Responsible AI for RAG & LLM agents

ğŸ”œ Privacy-preserving AI (Differential Privacy, Federated Learning)

ğŸ‘©â€ğŸ’» Contributors

Shraddha Sharma